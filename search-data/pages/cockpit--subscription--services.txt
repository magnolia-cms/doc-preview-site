---
title: "Subscription Services Help"
url: https://docs.magnolia-cms.com/cockpit/subscription/services/
category: DX Cloud
version: cloud
---

# Subscription Services Help

This section provides documentation for subscription service metrics and detailed performance metrics.

## [](#_availability_sla)availability-sla

**Availability**

This shows your subscribed Service Level Agreement for service availability. SLA values represent the guaranteed uptime percentage committed in your service agreement.

-   **Measurement:** Guaranteed uptime percentage

-   **Contract:** Your agreed service level commitment

-   **Monitoring:** Tracked against actual service performance


SLA metrics provide the baseline against which your actual service performance is measured for compliance and service credit calculations.

## [](#_traffic_sla)traffic-sla

**Traffic**

This displays your subscribed data transfer limits as defined in your service agreement. These values represent your contracted traffic allowances.

-   **Measurement:** Contracted data transfer limits

-   **Allowance:** Your subscribed traffic capacity

-   **Billing:** Baseline for usage calculations


Traffic SLA defines your committed data transfer capacity across all services in your subscription plan.

## [](#_roundtrip_sla)roundtrip-sla

**Response Time**

This shows your contracted response time commitments as specified in your service level agreement. SLA values represent the maximum response time guarantees.

-   **Measurement:** Guaranteed maximum response time in milliseconds

-   **Contract:** Your agreed performance commitment

-   **Monitoring:** Baseline for response time compliance


Response time SLA defines the performance standards your services are committed to maintain for optimal user experience.

## [](#_availability_cdn)availability-cdn

**CDN Availability**

This metric shows the uptime percentage for your CDN (Content Delivery Network) service. It measures how often your content delivery network is accessible to users worldwide.

-   **Expected:** Your subscribed SLA level (typically 99.9% or higher)

-   **Current:** Actual performance over the selected time period

-   **Monitoring:** Measured from multiple global locations every minute


The CDN availability directly impacts your website’s performance for end users. Lower availability means users may experience slower load times or inability to access your content.

## [](#_availability_ingress)availability-ingress

**Ingress Availability**

This shows the uptime percentage for your ingress load balancer. It measures how accessible your Magnolia instances are through the ingress gateway.

-   **Expected:** Your subscribed SLA level

-   **Current:** Actual performance over the selected time period

-   **Impact:** Affects all traffic routing to your Magnolia instances


The ingress serves as the entry point for all requests to your Magnolia CMS. High availability here is crucial for both author and public instances.

## [](#_availability_magnolia)availability-magnolia

**Magnolia Instance Availability**

This displays the uptime percentage for your Magnolia CMS instances. It measures how often your author and public instances are running and accessible.

-   **Expected:** Your subscribed SLA level

-   **Current:** Actual performance over the selected time period

-   **Includes:** Both author and public instance availability


This metric reflects the core availability of your Magnolia CMS application, including the author interface and public website delivery.

## [](#_traffic_cdn)traffic-cdn

**CDN Traffic (including Ingress)**

This shows the combined data transfer volume for both your CDN service and ingress traffic. The number displayed represents the total of CDN and ingress data transfer already summed together.

Traffic is measured in TB and includes all data flowing through both your content delivery network and ingress load balancer.

-   **Measurement:** Combined total data transfer in TB (CDN + Ingress)

-   **CDN portion:** Cache hits, cache misses, and origin requests

-   **Ingress portion:** All traffic routed through your load balancer

-   **Billing Impact:** May affect costs if you exceed your plan limits


This combined metric gives you a complete view of all external data transfer for your subscription. High values typically indicate good traffic distribution and content delivery performance across both CDN and ingress systems.

## [](#_traffic_ingress)traffic-ingress

**Ingress Traffic**

This displays the data transfer volume through your ingress load balancer. All traffic to your Magnolia instances passes through this ingress point.

-   **Measurement:** Total data transfer in TB

-   **Includes:** All HTTP/HTTPS requests to your instances

-   **Monitoring:** Helps track overall system load


## [](#_roundtrip_cdn)roundtrip-cdn

**CDN round trip**

This measures the average response time for requests served by your CDN. Lower values indicate better performance for your users.

-   **Measurement:** Average response time in milliseconds

-   **Global:** Measured from multiple worldwide locations

-   **Target:** Typically under 100ms for optimal user experience


Fast CDN response times ensure users worldwide get quick access to your content.

## [](#_roundtrip_ingress)roundtrip-ingress

**Ingress round trip**

This shows the average response time for requests to your ingress load balancer. This includes the time to route requests to your Magnolia instances.

-   **Measurement:** Average response time in milliseconds

-   **Includes:** Load balancing and routing time

-   **Impact:** Affects overall page load performance


## [](#_domain)domain

**Domain Configuration**

This shows the number of configured domains and their status for your subscription.

-   **Total Domains:** Number of domains configured

-   **Status:** Active/inactive domain count

-   **Configuration:** Includes ingress rules and certificates


Domain configuration affects how users access your Magnolia CMS instances and websites.

## [](#_metric_cdn_averagehitratio)metric.cdn.averageHitRatio

**Hit ratio**

The percentage of requests served from CDN cache versus fetched from origin.

-   **What is this?** The ratio of requests served from cached content compared to total requests. Higher is better (typically 85-95% is good).

-   **Why it matters:** A high cache hit ratio reduces load on your origin servers, improves response times, and lowers bandwidth costs.

-   **How to fix it:** Improve cache hit ratio by adjusting cache TTL settings, ensuring proper cache headers from origin, identifying uncacheable content, and reviewing purge/invalidation frequency.


## [](#_metric_cdn_hitratio)metric.cdn.hitRatio

**Hit ratio over time**

Timeline visualization of cache hit ratio across the selected time period.

-   **What is this?** A graph showing how effectively your CDN cache is being utilized over time, displayed as a percentage for each interval.

-   **Why it matters:** Monitoring cache hit ratio trends helps identify configuration issues, content changes that affect cacheability, or opportunities to improve performance.

-   **How to fix it:** If you see sudden drops, investigate recent configuration changes, origin server cache header modifications, or increased dynamic content requests.


## [](#_metric_cdn_averagecachecoverage)metric.cdn.averageCacheCoverage

**Cache coverage**

The percentage of content that is cacheable by the CDN.

-   **What is this?** The ratio of cacheable content to total content served, indicating how much of your traffic can be cached.

-   **Why it matters:** Higher cache coverage means more content can be served quickly from cache, reducing origin load and improving user experience.

-   **How to fix it:** Increase cache coverage by making more content cacheable through proper cache headers, reducing dynamic content, and implementing cache-friendly URL structures.


## [](#_metric_cdn_cachecoverage)metric.cdn.cacheCoverage

**Cache coverage over time**

Timeline showing cache coverage trends across the selected period.

-   **What is this?** A time-series graph displaying the percentage of cacheable content at regular intervals.

-   **Why it matters:** Understanding cache coverage patterns helps identify when content became less cacheable and optimize caching strategies.

-   **How to fix it:** Investigate drops in coverage by reviewing recent content changes, API implementations, or personalization features that may reduce cacheability.


## [](#_metric_cdn_totalcachestatuses)metric.cdn.totalCacheStatuses

**Cache statuses**

Distribution of different cache status types (HIT, MISS, PASS, etc.).

-   **What is this?** A breakdown showing how many requests resulted in cache hits, misses, bypasses, and other cache behaviors.

-   **Why it matters:** Understanding cache status distribution helps optimize caching strategy and identify issues with cache configuration.

-   **How to fix it:** High MISS rates suggest short TTLs or frequent content changes. High PASS rates indicate too much uncacheable content. Review cache policies and origin headers.


## [](#_metric_cdn_cachestatuses)metric.cdn.cacheStatuses

**Cache statuses over time**

Timeline showing cache status distribution across the selected period.

-   **What is this?** A time-series graph displaying how cache statuses have changed over time.

-   **Why it matters:** Tracking cache status trends helps identify when cache performance degraded and measure optimization impact.

-   **How to fix it:** Correlate changes in cache status with deployments, traffic patterns, or configuration updates to identify root causes.


## [](#_metric_cdn_averageavailability)metric.cdn.averageAvailability

**Availability**

The uptime percentage for your CDN service at the CDN level.

-   **What is this?** The percentage of successful requests at the CDN level, calculated from the ratio of 502/503 errors compared to total requests. This measures CDN availability, not origin server availability.

-   **Why it matters:** If your CDN availability drops, your users will experience failed requests or inability to access your content.

-   **How to fix it:** Check your CDN configuration, review error logs, and verify that your origin servers are healthy. Contact support if availability remains low.


## [](#_metric_cdn_availability)metric.cdn.availability

**Availability over time**

Timeline showing CDN uptime percentage across the selected period.

-   **What is this?** A time-series graph showing how your CDN availability has changed over time.

-   **Why it matters:** Identifying patterns or specific time periods of reduced availability helps diagnose intermittent issues or correlate problems with deployments.

-   **How to fix it:** Use this timeline to identify when availability drops occurred, then correlate with deployment logs, traffic patterns, or external incidents.


## [](#_metric_cdn_requestsperzone)metric.cdn.requestsPerZone

**Requests per zone**

Distribution of requests across different CDN edge locations.

-   **What is this?** A breakdown showing which geographic zones or edge locations are handling your CDN traffic.

-   **Why it matters:** Understanding geographic distribution helps with capacity planning and identifying regions with high demand or potential issues.

-   **How to fix it:** If certain zones show unexpectedly high or low traffic, investigate regional routing, DNS configuration, or potential geographic performance issues.


## [](#_metric_cdn_totalrequests)metric.cdn.totalRequests

**Requests**

Total number of requests handled by your CDN.

-   **What is this?** The cumulative count of all HTTP/HTTPS requests processed by the CDN for your content.

-   **Why it matters:** Understanding request volume helps with capacity planning and identifying unusual traffic patterns or potential DDoS attacks.

-   **How to fix it:** If requests are unexpectedly high, investigate for potential abuse, bot traffic, or viral content. If low, check if content is being cached properly.


## [](#_metric_cdn_requests)metric.cdn.requests

**CDN requests**

Request volume and patterns across your CDN.

-   **What is this?** Analysis of HTTP/HTTPS requests processed by the CDN, showing either traffic patterns over time or top requests ranked by frequency, path, or other dimensions.

-   **Why it matters:** Understanding request patterns helps identify peak usage times, popular content, traffic trends, and anomalies that may require attention.

-   **How to fix it:** Use this data to optimize caching strategies during peak times, prioritize improvements for high-traffic paths, or investigate unexpected spikes that could indicate issues.


## [](#_metric_cdn_totalrequestsperhost)metric.cdn.totalRequestsPerHost

**Requests per host**

Distribution of requests across different hostnames served by the CDN.

-   **What is this?** A breakdown showing which domains or subdomains are receiving the most CDN traffic.

-   **Why it matters:** Understanding per-host traffic helps identify which sites or applications are most active and may need optimization.

-   **How to fix it:** If a host has unexpectedly high traffic, investigate for issues like recursive requests, bot activity, or content that should be cached.


## [](#_metric_cdn_requestsperhost)metric.cdn.requestsPerHost

**Requests per host over time**

Timeline showing request distribution across hosts over the selected period.

-   **What is this?** A time-series graph displaying how traffic is distributed across different hosts over time.

-   **Why it matters:** Tracking per-host patterns helps identify traffic shifts, measure the impact of migrations, or detect anomalies for specific sites.

-   **How to fix it:** Investigate sudden changes in host-specific traffic by reviewing recent deployments, DNS changes, or marketing campaigns.


## [](#_metric_cdn_requestsrate)metric.cdn.requestsRate

**Requests rate over time**

The number of requests processed per unit of time.

-   **What is this?** The velocity of incoming requests, typically measured as requests per second (RPS) or requests per minute.

-   **Why it matters:** Request rate indicates system load and helps determine if your CDN can handle current traffic levels.

-   **How to fix it:** If request rate is consistently high, ensure adequate CDN capacity. For sudden spikes, investigate if they’re legitimate traffic or potential attacks.


## [](#_metric_cdn_averagertt)metric.cdn.averageRtt

**Round trip**

Average round-trip time for requests through the CDN.

-   **What is this?** The mean time (in milliseconds) for a complete request-response cycle through the CDN, including network latency.

-   **Why it matters:** Round-trip time directly impacts user experience. High RTT indicates network latency or slow CDN processing.

-   **How to fix it:** Improve RTT by using closer edge locations, optimizing CDN configuration, reducing payload sizes, or improving origin server response times.


## [](#_metric_cdn_rtt)metric.cdn.rtt

**Round trip over time**

Timeline showing round-trip time trends across the selected period.

-   **What is this?** A time-series graph displaying average RTT at regular intervals, showing performance patterns.

-   **Why it matters:** Tracking RTT trends helps identify performance degradation, measure optimization efforts, and detect geographic issues.

-   **How to fix it:** Investigate RTT spikes by correlating with traffic patterns, cache hit ratio changes, or origin server performance issues.


## [](#_metric_cdn_responsetime)metric.cdn.responseTime

**Response time over time**

Timeline showing response time trends for CDN-served content.

-   **What is this?** A graph displaying the average time taken to deliver content to users over the selected period.

-   **Why it matters:** Response time is a critical user experience metric. Tracking it helps ensure consistent performance.

-   **How to fix it:** Reduce response time by optimizing cache hit ratio, minimizing origin response time, enabling compression, and using appropriate edge locations.


## [](#_metric_cdn_totaltraffic)metric.cdn.totalTraffic

**Traffic**

Total amount of data transferred through your CDN.

-   **What is this?** The cumulative volume of data (usually in GB or TB) delivered from the CDN to end users during the selected period.

-   **Why it matters:** Traffic volume directly impacts CDN costs and helps predict infrastructure requirements. Unusual spikes may indicate content issues or attacks.

-   **How to fix it:** Optimize traffic by enabling compression, serving appropriately sized images, and ensuring efficient caching. Monitor for unexpected usage.


## [](#_metric_cdn_traffic)metric.cdn.traffic

**Traffic over time**

Timeline showing data transfer volume across the selected period.

-   **What is this?** A time-series graph displaying bandwidth consumption at regular intervals, showing data transfer patterns.

-   **Why it matters:** Understanding traffic patterns helps with cost management, capacity planning, and identifying content that consumes excessive bandwidth.

-   **How to fix it:** Identify traffic spikes and investigate associated content. Consider optimizing large files or implementing adaptive strategies.


## [](#_metric_cdn_trafficrate)metric.cdn.trafficRate

**Traffic rate**

The rate of data transfer through your CDN (e.g., MB/s or GB/hour).

-   **What is this?** The speed at which data is being transferred, typically measured as data volume per unit of time.

-   **Why it matters:** Traffic rate helps identify bandwidth constraints and whether your CDN can handle current throughput demands.

-   **How to fix it:** If consistently high, consider upgrading CDN capacity or optimizing content delivery. For spikes, investigate the source of high-bandwidth content.


## [](#_metric_cdn_trafficdetail)metric.cdn.trafficDetail

**Traffic detail over time**

Timeline showing data transfer broken down by body and header size.

-   **What is this?** A chart displaying bandwidth consumption over time, with traffic separated into body (content payload) and header (HTTP headers) components.

-   **Why it matters:** Understanding the breakdown between body and header traffic helps identify optimization opportunities. High header overhead may indicate excessive cookies, custom headers, or inefficient request patterns.

-   **How to fix it:** If header traffic is disproportionately high, review cookie sizes, remove unnecessary custom headers, and consider header compression. For high body traffic, optimize content through compression or caching.


## [](#_metric_cdn_totaltrafficdetail)metric.cdn.totalTrafficDetail

**Total traffic detail**

Total data transfer broken down by body and header size.

-   **What is this?** A summary showing cumulative bandwidth consumption separated into body (content payload) and header (HTTP headers) components for the selected period.

-   **Why it matters:** Seeing the total split between body and header traffic helps quantify the overhead of HTTP headers and prioritize optimization efforts.

-   **How to fix it:** Compare body versus header ratios to identify inefficiencies. A high header-to-body ratio suggests opportunities to reduce cookie sizes, consolidate requests, or streamline custom headers.


## [](#_metric_cdn_totalrequestspercode)metric.cdn.totalRequestsPerCode

**Requests per code**

Distribution of HTTP status codes returned by the CDN.

-   **What is this?** A breakdown showing the count of different HTTP status codes (200, 404, 500, etc.) returned to users.

-   **Why it matters:** Understanding status code distribution helps identify specific types of problems affecting users.

-   **How to fix it:** Address issues based on dominant error codes: 404 (missing content/routing), 502/503 (origin issues), 500 (application errors).


## [](#_metric_cdn_requestspercode)metric.cdn.requestsPerCode

**Requests per code over time**

Timeline showing HTTP status code distribution across the selected period.

-   **What is this?** A time-series graph displaying how status codes have changed over time.

-   **Why it matters:** Tracking status code patterns helps identify when errors began and correlate with deployments or configuration changes.

-   **How to fix it:** Use timeline to pinpoint when errors started, then review changes made around that timeframe.


## [](#_metric_cdn_errorratio)metric.cdn.errorRatio

**Errors ratio over time**

Timeline showing the percentage of failed requests across the selected period.

-   **What is this?** A graph displaying the ratio of error responses (4xx, 5xx) to total requests over time.

-   **Why it matters:** Error ratio trends help identify when problems began and whether they’re getting worse or improving.

-   **How to fix it:** Investigate error ratio spikes by checking origin server health, reviewing recent deployments, and analyzing specific error types.


## [](#_metric_cdn_errorratioperzone)metric.cdn.errorRatioPerZone

**Errors ratio per zone**

Distribution of error rates across different CDN edge locations.

-   **What is this?** A breakdown showing which geographic zones are experiencing higher error rates.

-   **Why it matters:** Identifying zone-specific errors helps isolate regional issues with CDN configuration or origin connectivity.

-   **How to fix it:** For zones with high errors, check regional CDN settings, origin server accessibility from that region, and network paths.


## [](#_metric_cdn_status3xx)metric.cdn.status3xx

**3xx over time**

Timeline showing redirect responses (HTTP 3xx codes) over the selected period.

-   **What is this?** A graph displaying the count of redirect status codes (301, 302, etc.) returned over time.

-   **Why it matters:** Excessive redirects can slow down user experience and indicate configuration issues.

-   **How to fix it:** Review redirect rules, eliminate redirect chains, and ensure proper canonical URL configuration.


## [](#_metric_cdn_status4xx)metric.cdn.status4xx

**4xx over time**

Timeline showing client error responses (HTTP 4xx codes) over the selected period.

-   **What is this?** A graph displaying client errors like 404 (Not Found), 403 (Forbidden), and 401 (Unauthorized) over time.

-   **Why it matters:** High 4xx rates indicate broken links, missing content, or access control issues affecting users.

-   **How to fix it:** For 404s, fix broken links or implement proper redirects. For 403s, review access controls and authentication.


## [](#_metric_cdn_status5xx)metric.cdn.status5xx

**5xx over time**

Timeline showing server error responses (HTTP 5xx codes) over the selected period.

-   **What is this?** A graph displaying server errors like 500 (Internal Server Error), 502 (Bad Gateway), and 503 (Service Unavailable) over time.

-   **Why it matters:** 5xx errors indicate serious problems with origin servers or CDN configuration that prevent users from accessing content.

-   **How to fix it:** Investigate origin server health, check CDN-to-origin connectivity, review recent deployments, and scale origin capacity if needed.


## [](#_metric_cdn_rttperzone)metric.cdn.rttPerZone

**Round trip per zone**

Distribution of round-trip times across different CDN edge locations.

-   **What is this?** A breakdown showing average RTT for each geographic zone or edge location.

-   **Why it matters:** Identifying zones with high RTT helps optimize CDN configuration and identify regional performance issues.

-   **How to fix it:** For zones with poor RTT, investigate network paths, consider additional edge locations, or review origin connectivity from that region.


## [](#_metric_cdn_averageresponsetimepercachestatus)metric.cdn.averageResponseTimePerCacheStatus

**Response time per cache status**

Average response times broken down by cache status (HIT, MISS, etc.).

-   **What is this?** A comparison showing how response times differ for cached versus uncached content.

-   **Why it matters:** Understanding response time by cache status helps quantify the performance benefit of caching.

-   **How to fix it:** Large differences indicate opportunities to improve cache hit ratio for better performance.


## [](#_metric_cdn_responsetimepercachestatus)metric.cdn.responseTimePerCacheStatus

**Response time per cache status over time**

Timeline showing response times for different cache statuses across the selected period.

-   **What is this?** A time-series graph displaying how response times vary by cache status over time.

-   **Why it matters:** Tracking response time by cache status helps identify when cache performance degraded.

-   **How to fix it:** If cache HIT response times increase, investigate edge location performance. If MISS times increase, check origin server health.


## [](#_metric_cdn_trafficperzone)metric.cdn.trafficPerZone

**Traffic per zone**

Distribution of data transfer across different CDN edge locations.

-   **What is this?** A breakdown showing which geographic zones are serving the most bandwidth.

-   **Why it matters:** Understanding geographic bandwidth distribution helps with capacity planning and cost optimization.

-   **How to fix it:** For zones with unexpectedly high traffic, investigate if content is being served from optimal locations or if there are regional issues.


## [](#_metric_cdn_averageresponsetimeperzone)metric.cdn.averageResponseTimePerZone

**Response time per zone**

Average response times for different CDN edge locations.

-   **What is this?** A comparison showing which geographic zones have the fastest or slowest response times.

-   **Why it matters:** Identifying slow zones helps optimize CDN configuration and improve user experience in specific regions.

-   **How to fix it:** For slow zones, consider additional edge locations, review network paths, or investigate regional origin connectivity issues.


## [](#_metric_cdn_requestsperprotocol)metric.cdn.requestsPerProtocol

**Requests per protocol**

Distribution of requests by protocol (HTTP, HTTPS, HTTP/2, etc.).

-   **What is this?** A breakdown showing which protocols are being used by clients accessing your CDN.

-   **Why it matters:** Understanding protocol usage helps optimize configuration and ensure users benefit from modern protocol features.

-   **How to fix it:** If many users still use HTTP, enforce HTTPS redirects. If HTTP/2 adoption is low, verify CDN configuration supports it.


## [](#_metric_ingress_averageavailability)metric.ingress.averageAvailability

**Availability**

The uptime percentage for your Ingress resources at the Ingress level.

-   **What is this?** The percentage of successful requests at the Ingress level, calculated from the ratio of 502/503 errors compared to total requests. This measures Ingress availability, not backend service availability.

-   **Why it matters:** If Ingress availability drops, your applications become inaccessible to users, directly impacting business operations.

-   **How to fix it:** Check Ingress controller health, verify backend service availability, review recent configuration changes, and ensure adequate Ingress controller capacity.


## [](#_metric_ingress_availability)metric.ingress.availability

**Availability over time**

Timeline showing Ingress uptime percentage across the selected period.

-   **What is this?** A time-series graph showing how your Ingress availability has changed over time.

-   **Why it matters:** Identifying patterns of reduced availability helps diagnose intermittent issues and correlate problems with deployments or traffic patterns.

-   **How to fix it:** Use this timeline to identify when availability drops occurred, then correlate with deployment history, configuration changes, or traffic spikes.


## [](#_metric_ingress_totalrequests)metric.ingress.totalRequests

**Requests**

Total number of HTTP/HTTPS requests handled by your Ingress.

-   **What is this?** The cumulative count of all incoming requests processed through your Ingress resources.

-   **Why it matters:** Request count is a key metric for understanding application usage, detecting unusual traffic patterns, and planning infrastructure capacity.

-   **How to fix it:** If request volume is unexpectedly high, check for bot activity, implement rate limiting, or review application logs for errors causing retries.


## [](#_metric_ingress_requests)metric.ingress.requests

**Ingress requests**

Request volume and patterns through your ingress controllers.

-   **What is this?** Analysis of HTTP/HTTPS requests processed by your ingress, showing either traffic patterns over time or top requests ranked by frequency, path, status code, or other dimensions.

-   **Why it matters:** Understanding request patterns helps identify peak usage times, popular endpoints, measure deployment impact, and detect anomalies like sudden spikes or retry storms.

-   **How to fix it:** Use this data to optimize application performance for high-traffic endpoints, investigate unexpected patterns, or correlate request behavior with application issues.


## [](#_metric_ingress_requestsrate)metric.ingress.requestsRate

**Requests rate over time**

Timeline showing the number of requests per unit of time.

-   **What is this?** The velocity of incoming requests, typically measured as requests per second (RPS) or requests per minute.

-   **Why it matters:** Request rate indicates system load and helps determine if your infrastructure can handle current traffic levels.

-   **How to fix it:** If request rate is consistently high, implement rate limiting, enable caching, optimize slow endpoints, or scale Ingress controllers.


## [](#_metric_ingress_requestsperprotocol)metric.ingress.requestsPerProtocol

**Requests per protocol**

Distribution of requests by protocol (HTTP, HTTPS, HTTP/2, etc.).

-   **What is this?** A breakdown showing which protocols clients are using to access your applications.

-   **Why it matters:** Understanding protocol usage helps optimize configuration, ensure security (HTTPS adoption), and leverage modern protocol features.

-   **How to fix it:** If many requests use HTTP, enforce HTTPS redirects. If HTTP/2 adoption is low, verify Ingress supports it and clients can negotiate it.


## [](#_metric_ingress_averagertt)metric.ingress.averageRtt

**Round trip**

Average round-trip time for requests through your Ingress.

-   **What is this?** The mean time (in milliseconds) for a complete request-response cycle through Ingress, including backend processing.

-   **Why it matters:** Round-trip time directly impacts user experience. High RTT indicates network latency or slow backend services.

-   **How to fix it:** Improve RTT by optimizing backend services, adding caching layers, improving network configuration, or scaling infrastructure.


## [](#_metric_ingress_rtt)metric.ingress.rtt

**Round trip over time**

Timeline showing round-trip time trends across the selected period.

-   **What is this?** A time-series graph displaying average RTT at regular intervals, showing performance patterns.

-   **Why it matters:** Tracking RTT trends helps identify performance degradation, measure optimization impact, and detect infrastructure issues.

-   **How to fix it:** Investigate RTT spikes by reviewing backend service performance, checking network issues, and correlating with traffic patterns.


## [](#_metric_ingress_responsetime)metric.ingress.responseTime

**Response time over time**

Timeline showing response time trends for requests through Ingress.

-   **What is this?** A graph displaying the average time taken for backend services to respond through Ingress over the selected period.

-   **Why it matters:** Response time is a critical user experience metric indicating how quickly your applications serve requests.

-   **How to fix it:** Reduce response time by optimizing backend services, adding caching, fixing slow database queries, or scaling backend pods.


## [](#_metric_ingress_totaltraffic)metric.ingress.totalTraffic

**Traffic**

Total amount of data transferred through your Ingress resources.

-   **What is this?** The cumulative volume of data (typically in GB or TB) that has passed through your Ingress.

-   **Why it matters:** Traffic volume indicates usage patterns, helps with capacity planning, and can reveal unusual activity requiring investigation.

-   **How to fix it:** If traffic is unexpectedly high, check for misconfigured applications, bot traffic, or content that should be cached but isn’t.


## [](#_metric_ingress_traffic)metric.ingress.traffic

**Traffic over time**

Timeline showing data transfer volume across the selected period.

-   **What is this?** A time-series graph displaying network traffic at regular intervals, showing transfer patterns.

-   **Why it matters:** Understanding traffic patterns helps identify peak usage times and correlate traffic with application deployments or issues.

-   **How to fix it:** Use this data to optimize resource allocation during peak times and investigate unexpected spikes.


## [](#_metric_ingress_trafficrate)metric.ingress.trafficRate

**Traffic rate**

The rate of data transfer through your Ingress (e.g., MB/s).

-   **What is this?** The speed at which data is being transferred through Ingress, measured as data volume per unit of time.

-   **Why it matters:** Traffic rate helps identify bandwidth constraints and whether infrastructure can handle current load.

-   **How to fix it:** If traffic rate is consistently high, consider scaling Ingress controllers, optimizing response sizes, or implementing caching.


## [](#_metric_ingress_totalrequestspercode)metric.ingress.totalRequestsPerCode

**Requests per code**

Distribution of HTTP status codes returned by your services through Ingress.

-   **What is this?** A breakdown showing the count of different HTTP status codes (200, 404, 500, etc.).

-   **Why it matters:** Understanding status code distribution helps identify specific types of problems affecting users.

-   **How to fix it:** Address issues based on error codes: 404 (routing/path issues), 502/503 (backend unavailable), 500 (application errors).


## [](#_metric_ingress_requestspercode)metric.ingress.requestsPerCode

**Requests per code over time**

Timeline showing HTTP status code distribution across the selected period.

-   **What is this?** A time-series graph displaying how status codes have changed over time.

-   **Why it matters:** Tracking status code patterns helps identify when errors began and correlate with deployments.

-   **How to fix it:** Use timeline to pinpoint when errors started, then review deployment history and backend service logs.


## [](#_metric_ingress_errorratio)metric.ingress.errorRatio

**Errors ratio over time**

Timeline showing the percentage of failed requests across the selected period.

-   **What is this?** A graph displaying the ratio of error responses (4xx, 5xx) to total requests over time.

-   **Why it matters:** Error ratio trends help identify when problems began and measure recovery efforts.

-   **How to fix it:** Investigate error spikes by checking backend service health, reviewing recent deployments, and analyzing specific error types.


## [](#_metric_ingress_status3xx)metric.ingress.status3xx

**3xx over time**

Timeline showing redirect responses (HTTP 3xx codes) over the selected period.

-   **What is this?** A graph displaying the count of redirect status codes (301, 302, etc.) over time.

-   **Why it matters:** Excessive redirects can slow user experience and indicate configuration issues.

-   **How to fix it:** Review Ingress redirect rules, eliminate redirect chains, and ensure proper routing configuration.


## [](#_metric_ingress_status4xx)metric.ingress.status4xx

**4xx over time**

Timeline showing client error responses (HTTP 4xx codes) over the selected period.

-   **What is this?** A graph displaying client errors like 404 (Not Found), 403 (Forbidden), and 401 (Unauthorized) over time.

-   **Why it matters:** High 4xx rates indicate routing issues, missing endpoints, or authentication/authorization problems.

-   **How to fix it:** For 404s, fix Ingress path rules or missing backend services. For 403/401, review authentication configuration.


## [](#_metric_ingress_status5xx)metric.ingress.status5xx

**5xx over time**

Timeline showing server error responses (HTTP 5xx codes) over the selected period.

-   **What is this?** A graph displaying server errors like 500 (Internal Server Error), 502 (Bad Gateway), and 503 (Service Unavailable) over time.

-   **Why it matters:** 5xx errors indicate serious problems with backend services or Ingress configuration preventing users from accessing applications.

-   **How to fix it:** Investigate backend service health, check pod logs, review recent deployments, scale backend pods if needed, and verify resource limits.


## [](#_metric_redirect_averageredirectratio)metric.redirect.averageRedirectRatio

**Redirect ratio**

The percentage of requests that resulted in redirects.

-   **What is this?** The ratio of redirect responses (3xx status codes) to total requests processed by your redirect rules.

-   **Why it matters:** Understanding redirect ratio helps verify that your redirect rules are working as expected and identify potential misconfigurations.

-   **How to fix it:** If the ratio is unexpectedly high or low, review your redirect rules configuration and verify that rules are matching the intended traffic patterns.


## [](#_metric_redirect_redirectratio)metric.redirect.redirectRatio

**Redirect ratio over time**

Timeline showing redirect ratio trends across the selected period.

-   **What is this?** A time-series graph displaying the percentage of requests resulting in redirects at regular intervals.

-   **Why it matters:** Tracking redirect ratio over time helps identify when redirect behavior changed, correlate with rule deployments, and detect anomalies.

-   **How to fix it:** Investigate sudden changes by reviewing recent redirect rule modifications or changes in incoming traffic patterns.


## [](#_metric_redirect_averageavailability)metric.redirect.averageAvailability

**Redirect availability**

The uptime percentage for your redirect service at the redirect level.

-   **What is this?** The percentage of successful requests at the redirect level, calculated from the ratio of 502/503 errors compared to total requests. This measures redirect service availability, not upstream backend availability.

-   **Why it matters:** If redirect availability drops, users may experience failed redirects or be unable to reach their intended destinations.

-   **How to fix it:** Verify the number of redirect servers available or consider increasing the number of replicas to improve redundancy and availability.


## [](#_metric_redirect_availability)metric.redirect.availability

**Redirect availability over time**

Timeline showing redirect service uptime across the selected period.

-   **What is this?** A time-series graph showing how your redirect service availability has changed over time, based on the 502/503 error ratio.

-   **Why it matters:** Identifying patterns of reduced availability helps diagnose intermittent issues and correlate problems with configuration changes or capacity constraints.

-   **How to fix it:** Use this timeline to identify when availability drops occurred, then verify redirect server capacity and consider scaling replicas if needed.


## [](#_metric_redirect_totalrequests)metric.redirect.totalRequests

**Redirect requests**

Total number of requests processed by the redirect server.

-   **What is this?** The cumulative count of all requests passing through the redirect server, including both redirect responses and requests proxied to upstream backends.

-   **Why it matters:** Request count helps understand redirect server usage, detect unusual traffic patterns, and plan capacity. Note that only traffic configured to route through the redirect server is counted here, not all Ingress traffic.

-   **How to fix it:** If request volume is unexpected, review your Ingress configuration to verify which hosts route traffic through the redirect server.


## [](#_metric_redirect_requests)metric.redirect.requests

**Redirect requests over time**

Timeline showing request volume through the redirect server across the selected period.

-   **What is this?** A time-series graph displaying the number of requests passing through the redirect server at regular intervals, including both redirects and proxied requests.

-   **Why it matters:** Request patterns help identify peak usage times, measure the impact of configuration changes, and detect anomalies.

-   **How to fix it:** Use this timeline to correlate redirect server traffic with application behavior and identify unexpected patterns.


## [](#_metric_redirect_requestsrate)metric.redirect.requestsRate

**Redirect requests rate over time**

Timeline showing the number of requests per unit of time through the redirect server.

-   **What is this?** The velocity of requests passing through the redirect server, typically measured as requests per second or per minute.

-   **Why it matters:** Request rate indicates load on your redirect server and helps determine if capacity can handle current traffic.

-   **How to fix it:** If request rate is consistently high, consider scaling redirect server replicas.


## [](#_metric_redirect_requestsperprotocol)metric.redirect.requestsPerProtocol

**Redirect requests per protocol**

Distribution of requests through the redirect server by protocol.

-   **What is this?** A breakdown showing the protocol (HTTP, HTTPS, etc.) used by clients for requests passing through the redirect server. Note that protocol termination and upgrades are handled at the Ingress level, not by the redirect server.

-   **Why it matters:** Understanding the protocol distribution of redirect server traffic helps monitor traffic patterns.

-   **How to fix it:** To enforce HTTPS or manage protocol upgrades, configure these at the Ingress level rather than on the redirect server.


## [](#_metric_redirect_responsetime)metric.redirect.responseTime

**Redirect response time over time**

Timeline showing response time trends for the redirect server.

-   **What is this?** A graph displaying the average time taken to process requests through the redirect server, including both redirect responses and requests proxied to upstream backends.

-   **Why it matters:** Response time affects user experience. Fast processing ensures seamless navigation for redirects and proxied requests.

-   **How to fix it:** If response times are high, review redirect rule complexity, check for chained redirects, and investigate upstream backend performance for proxied requests.


## [](#_metric_redirect_totaltraffic)metric.redirect.totalTraffic

**Redirect traffic**

Total amount of data transferred through the redirect server.

-   **What is this?** The cumulative volume of data passing through the redirect server during the selected period, including both redirect responses and proxied traffic.

-   **Why it matters:** Traffic volume helps understand bandwidth usage and identify unusual patterns.

-   **How to fix it:** If traffic is unexpectedly high, investigate redirect loops or high-bandwidth proxied requests passing through the redirect server.


## [](#_metric_redirect_traffic)metric.redirect.traffic

**Redirect traffic over time**

Timeline showing data transfer volume through the redirect server across the selected period.

-   **What is this?** A time-series graph displaying redirect server traffic at regular intervals, including both redirect responses and proxied traffic.

-   **Why it matters:** Understanding traffic patterns helps identify peak times and correlate with configuration changes.

-   **How to fix it:** Use this data to investigate unexpected traffic spikes and optimize redirect server capacity.


## [](#_metric_redirect_trafficrate)metric.redirect.trafficRate

**Redirect traffic rate**

The rate of data transfer through the redirect server (e.g., MB/s).

-   **What is this?** The speed at which data is being transferred through the redirect server, measured per unit of time.

-   **Why it matters:** Traffic rate helps identify bandwidth usage patterns for your redirect server.

-   **How to fix it:** If traffic rate is consistently high, review traffic patterns and consider scaling redirect server capacity.


## [](#_metric_redirect_totalrequestspercode)metric.redirect.totalRequestsPerCode

**Redirect requests per code**

Distribution of HTTP status codes returned through the redirect server.

-   **What is this?** A breakdown showing the count of different HTTP status codes returned, including redirect codes (301, 302, 307, 308) and responses from proxied upstream backends.

-   **Why it matters:** Understanding status code distribution helps verify redirect configuration and monitor proxied request health.

-   **How to fix it:** For redirect codes, ensure appropriate types (301 for permanent, 302/307 for temporary). For other codes, investigate upstream backend behavior.


## [](#_metric_redirect_requestspercode)metric.redirect.requestsPerCode

**Redirect requests per code over time**

Timeline showing HTTP status code distribution through the redirect server across the selected period.

-   **What is this?** A time-series graph displaying how status codes have changed over time, including both redirect responses and proxied request responses.

-   **Why it matters:** Tracking status code patterns helps identify when behavior changed and measure the impact of configuration modifications.

-   **How to fix it:** Use timeline to pinpoint when status code distributions changed, then review redirect rule deployments or upstream backend changes.


## [](#_metric_redirect_errorratio)metric.redirect.errorRatio

**Redirect errors ratio over time**

Timeline showing the percentage of failed requests through the redirect server across the selected period.

-   **What is this?** A graph displaying the ratio of error responses (4xx, 5xx) to total requests passing through the redirect server.

-   **Why it matters:** Error ratio trends help identify when problems began and whether they’re improving or worsening.

-   **How to fix it:** Investigate error spikes by checking redirect rule syntax, verifying upstream backend health, and reviewing redirect server capacity.


## [](#_metric_redirect_status3xx)metric.redirect.status3xx

**Redirect 3xx over time**

Timeline showing successful redirect responses (HTTP 3xx codes) over the selected period.

-   **What is this?** A graph displaying the count of successful redirect status codes (301, 302, 307, 308) over time.

-   **Why it matters:** Tracking successful redirects helps verify rules are working and measure redirect volume trends.

-   **How to fix it:** If 3xx counts are lower than expected, review redirect rule patterns to ensure they’re matching intended traffic.


## [](#_metric_redirect_status4xx)metric.redirect.status4xx

**Redirect 4xx over time**

Timeline showing client error responses (HTTP 4xx codes) through the redirect server over the selected period.

-   **What is this?** A graph displaying client errors like 404 (not found) or 400 (bad request) returned through the redirect server.

-   **Why it matters:** High 4xx rates may indicate redirect rules pointing to invalid targets or issues with proxied upstream backends.

-   **How to fix it:** For redirect-related 404s, verify target URLs exist. For proxied requests, investigate upstream backend routing and configuration.


## [](#_metric_redirect_status5xx)metric.redirect.status5xx

**Redirect 5xx over time**

Timeline showing server error responses (HTTP 5xx codes) through the redirect server over the selected period.

-   **What is this?** A graph displaying server errors like 500 (Internal Server Error) or 502 (Bad Gateway) returned through the redirect server.

-   **Why it matters:** 5xx errors indicate serious problems with the redirect server or upstream backends preventing successful request processing.

-   **How to fix it:** Verify redirect server capacity, check upstream backend health, review logs, and consider increasing the number of replicas.


## [](#_metric_mgnldatabase_latestsize)metric.mgnlDatabase.latestSize

**Size**

The current total size of your Magnolia database.

-   **What is this?** The complete storage footprint of your PostgreSQL database containing all Magnolia JCR data.

-   **Why it matters:** Database size affects backup times, restore operations, and may impact query performance as it grows.

-   **How to fix it:** If size is growing unexpectedly, review content versioning policies, check for orphaned binaries, and consider archiving old content.


## [](#_metric_mgnldatabase_size)metric.mgnlDatabase.size

**Size over time**

Timeline showing database size trends across the selected period.

-   **What is this?** A time-series graph displaying how your database size has changed over time.

-   **Why it matters:** Tracking size trends helps predict storage needs, identify unexpected growth, and plan capacity.

-   **How to fix it:** Investigate sudden growth by reviewing recent content imports, publication activity, or versioning behavior.


## [](#_metric_mgnldatabase_latestblobsize)metric.mgnlDatabase.latestBlobSize

**Blob size**

The current total size of binary data (blobs) stored in your database.

-   **What is this?** The storage used by large records in Magnolia, typically assets like images, PDFs, videos, and other documents.

-   **Why it matters:** Blob storage often represents the largest portion of database size and directly impacts storage costs and backup times.

-   **How to fix it:** If blob size is high, review asset management practices, consider external DAM integration, or archive unused assets.


## [](#_metric_mgnldatabase_blobsize)metric.mgnlDatabase.blobSize

**Blob size over time**

Timeline showing binary storage trends across the selected period.

-   **What is this?** A time-series graph displaying how blob storage has changed over time.

-   **Why it matters:** Tracking blob size trends helps identify when large assets were added and plan for storage capacity.

-   **How to fix it:** Correlate growth spikes with content uploads to identify sources of large binary data.


## [](#_metric_mgnldatabase_latestblobavgsize)metric.mgnlDatabase.latestBlobAvgSize

**Average blob size**

The current average size of individual binary records.

-   **What is this?** The mean size of blob entries, indicating the typical size of assets stored in your Magnolia instance.

-   **Why it matters:** Average blob size helps understand your content profile and identify if unusually large assets are being stored.

-   **How to fix it:** If average size is high, review image optimization settings, implement asset size guidelines, or enable automatic image resizing.


## [](#_metric_mgnldatabase_blobavgsize)metric.mgnlDatabase.blobAvgSize

**Average blob size over time**

Timeline showing average binary size trends across the selected period.

-   **What is this?** A time-series graph displaying how the average blob size has changed over time.

-   **Why it matters:** Tracking average size trends helps identify changes in content patterns or asset management practices.

-   **How to fix it:** Investigate increases by reviewing recently uploaded assets and content creation workflows.


## [](#_metric_mgnldatabase_latestcacheratio)metric.mgnlDatabase.latestCacheRatio

**Cache ratio**

The current database cache hit ratio.

-   **What is this?** The percentage of database queries served from PostgreSQL’s shared buffer cache rather than reading from disk.

-   **Why it matters:** A higher cache ratio indicates better performance, as cached queries are significantly faster than disk reads.

-   **How to fix it:** If cache ratio is low, consider increasing PostgreSQL shared\_buffers, optimizing frequently accessed queries, or reviewing JCR workspace structure.


## [](#_metric_mgnldatabase_cacheratio)metric.mgnlDatabase.cacheRatio

**Cache ratio over time**

Timeline showing cache performance trends across the selected period.

-   **What is this?** A time-series graph displaying how the database cache hit ratio has changed over time.

-   **Why it matters:** Tracking cache ratio trends helps identify performance degradation and measure the impact of optimizations.

-   **How to fix it:** Investigate drops in cache ratio by correlating with traffic patterns, new content types, or configuration changes.


## [](#_metric_mgnlworkspace_latesttotal)metric.mgnlWorkspace.latestTotal

**Total**

The current number of records in the workspace.

-   **What is this?** The total count of rows stored in the PostgreSQL table for this JCR workspace.

-   **Why it matters:** Record count indicates workspace usage and can affect query performance as it grows.

-   **How to fix it:** If record count is unexpectedly high, review content structure, check for excessive versioning, or consider archiving old content.


## [](#_metric_mgnlworkspace_total)metric.mgnlWorkspace.total

**Total over time**

Timeline showing record count trends for the selected workspace.

-   **What is this?** A time-series graph displaying how the number of records in this workspace has changed over time.

-   **Why it matters:** Tracking record growth helps predict capacity needs and identify unexpected content creation patterns.

-   **How to fix it:** Investigate sudden increases by reviewing publication activity, content imports, or automated content creation.


## [](#_metric_mgnlworkspace_latesttotalsize)metric.mgnlWorkspace.latestTotalSize

**Total size**

The current complete storage size of the workspace.

-   **What is this?** Everything PostgreSQL stores for this workspace table, including table data, indexes, TOAST data, and free space maps.

-   **Why it matters:** Total size represents the full storage footprint and affects backup times and storage costs.

-   **How to fix it:** If size is larger than expected, review content stored in this workspace, check for bloat, and consider running VACUUM.


## [](#_metric_mgnlworkspace_totalsize)metric.mgnlWorkspace.totalSize

**Total size over time**

Timeline showing total workspace size trends.

-   **What is this?** A time-series graph displaying how the complete storage footprint of this workspace has changed over time.

-   **Why it matters:** Tracking total size helps plan storage capacity and identify workspaces with rapid growth.

-   **How to fix it:** Correlate size increases with content activity to understand growth drivers.


## [](#_metric_mgnlworkspace_latesttablesize)metric.mgnlWorkspace.latestTableSize

**Table size**

The current size of table data pages for the workspace.

-   **What is this?** The size of actual table data including the visibility map and free space map, but excluding indexes and TOAST data.

-   **Why it matters:** Table size indicates how much raw content data is stored, separate from indexing overhead.

-   **How to fix it:** If table size is disproportionately large compared to record count, check for large text fields or inefficient content structure.


## [](#_metric_mgnlworkspace_tablesize)metric.mgnlWorkspace.tableSize

**Table size over time**

Timeline showing table size trends for the selected workspace.

-   **What is this?** A time-series graph displaying how the table data size has changed over time.

-   **Why it matters:** Tracking table size separately from total size helps identify whether growth is from data or indexes.

-   **How to fix it:** Compare with index size trends to understand the balance between data and indexing overhead.


## [](#_metric_mgnlworkspace_latestindexsize)metric.mgnlWorkspace.latestIndexSize

**Index size**

The current size of indexes for the workspace.

-   **What is this?** The storage used by all indexes associated with this workspace’s PostgreSQL table.

-   **Why it matters:** Index size affects query performance and storage costs. Indexes speed up queries but consume space.

-   **How to fix it:** If index size is disproportionately large, review custom indexes or consider index maintenance operations.


## [](#_metric_mgnlworkspace_indexsize)metric.mgnlWorkspace.indexSize

**Index size over time**

Timeline showing index size trends for the selected workspace.

-   **What is this?** A time-series graph displaying how index storage has changed over time.

-   **Why it matters:** Tracking index growth helps identify indexing efficiency and plan for storage needs.

-   **How to fix it:** If index size grows faster than table size, investigate index bloat or consider REINDEX operations.


## [](#_metric_mgnlworkspace_latestnumdeadtuples)metric.mgnlWorkspace.latestNumDeadTuples

**Dead tuples**

The current number of dead tuples in the workspace.

-   **What is this?** Row versions that are no longer visible to any transaction but haven’t been cleaned up yet by PostgreSQL’s VACUUM process.

-   **Why it matters:** High dead tuple counts indicate table bloat, which can degrade query performance and waste storage.

-   **How to fix it:** If dead tuples are high, the database may benefit from a VACUUM operation. Consider reviewing autovacuum settings or running manual VACUUM.


## [](#_metric_mgnlworkspace_numdeadtuples)metric.mgnlWorkspace.numDeadTuples

**Dead tuples over time**

Timeline showing dead tuple trends for the selected workspace.

-   **What is this?** A time-series graph displaying how the number of dead tuples has changed over time.

-   **Why it matters:** Tracking dead tuple trends helps identify workspaces with high update/delete activity that may need more aggressive vacuuming.

-   **How to fix it:** If dead tuples consistently rise, review autovacuum configuration or schedule regular VACUUM operations during low-traffic periods.


## [](#_metric_mgnlpublication_totalpublications)metric.mgnlPublication.totalPublications

**Publications**

The total number of publication operations during the selected time period.

-   **What is this?** The count of content items published from Author to Public instances.

-   **Why it matters:** Publication volume indicates content activity levels and helps identify busy periods.


## [](#_metric_mgnlpublication_publications)metric.mgnlPublication.publications

**Publications over time**

Timeline showing publication activity trends.

-   **What is this?** A time-series graph displaying how publication operations have varied over the selected period.

-   **Why it matters:** Tracking publication trends helps identify peak activity times and unusual patterns.


## [](#_metric_mgnlpublication_totalunpublications)metric.mgnlPublication.totalUnpublications

**Unpublications**

The total number of unpublication operations during the selected time period.

-   **What is this?** The count of content items removed from Public instances.

-   **Why it matters:** Unpublication activity indicates content lifecycle management and helps track content removal patterns.


## [](#_metric_mgnlpublication_avgunpublishratio)metric.mgnlPublication.avgUnpublishRatio

**Unpublish ratio**

The ratio of unpublications to total publication operations.

-   **What is this?** A percentage showing how many operations were unpublications compared to the total.

-   **Why it matters:** A high unpublish ratio may indicate content churn or frequent corrections to published content.


## [](#_metric_mgnlpublication_unpublications)metric.mgnlPublication.unpublications

**Unpublications over time**

Timeline showing unpublication activity trends.

-   **What is this?** A time-series graph displaying how unpublication operations have varied over the selected period.

-   **Why it matters:** Tracking unpublication trends helps identify patterns in content removal activity.


## [](#_metric_mgnlpublication_avgsuccessratio)metric.mgnlPublication.avgSuccessRatio

**Success ratio**

The percentage of publication operations that completed successfully.

-   **What is this?** The proportion of publications that reached Public instances without errors.

-   **Why it matters:** A low success ratio indicates publishing problems that may prevent content from reaching your audience. Check the Top error messages metric to identify common failure causes.


## [](#_metric_mgnlpublication_successratio)metric.mgnlPublication.successRatio

**Success ratio over time**

Timeline showing success ratio trends.

-   **What is this?** A time-series graph displaying how the publication success rate has changed over time.

-   **Why it matters:** Tracking success ratio trends helps identify periods of publishing instability.


## [](#_metric_mgnlpublication_errormessages)metric.mgnlPublication.errorMessages

**Top error messages**

The most common error messages from failed publication operations.

-   **What is this?** A ranked list of error messages encountered during publishing, sorted by frequency.

-   **Why it matters:** Identifying common errors helps prioritize troubleshooting efforts and address the most impactful issues first.


## [](#_metric_mgnlpublication_avglatency)metric.mgnlPublication.avgLatency

**Latency**

The average time for publication operations to complete.

-   **What is this?** The mean duration from initiating a publication to confirmation of delivery to Public instances.

-   **Why it matters:** High latency affects editor productivity and can delay time-sensitive content updates.


## [](#_metric_mgnlpublication_latency)metric.mgnlPublication.latency

**Latency over time**

Timeline showing publication latency trends.

-   **What is this?** A time-series graph displaying how publication response times have varied over the selected period.

-   **Why it matters:** Tracking latency trends helps identify performance degradation or improvements over time.


## [](#_metric_mgnlpublication_totalp95latency)metric.mgnlPublication.totalP95Latency

**P95 latency**

The 95th percentile publication latency.

-   **What is this?** The response time threshold that 95% of publication operations fall within.

-   **Why it matters:** P95 latency reveals the experience of slower operations, excluding extreme outliers.


## [](#_metric_mgnlpublication_p95latency)metric.mgnlPublication.p95Latency

**P95 latency over time**

Timeline showing 95th percentile latency trends.

-   **What is this?** A time-series graph displaying how the P95 publication latency has changed over time.

-   **Why it matters:** Tracking P95 trends helps identify consistency issues that average latency might hide.


## [](#_metric_mgnlpublication_avgsubscriberresponsetime)metric.mgnlPublication.avgSubscriberResponseTime

**Subscriber response time**

Average response time for each subscriber instance.

-   **What is this?** A breakdown of publication latency by individual Public instance subscribers.

-   **Why it matters:** Identifies which subscribers are responding slowly and may need attention.


## [](#_metric_mgnlpublication_subscriberresponsetime)metric.mgnlPublication.subscriberResponseTime

**Subscriber response time over time**

Timeline showing response time trends for each subscriber.

-   **What is this?** A time-series graph displaying how each subscriber’s response time has varied over the selected period.

-   **Why it matters:** Tracking per-subscriber trends helps identify intermittent performance issues and whether problems are isolated or system-wide.


## [](#_metric_mgnlpublication_avgrollbackratio)metric.mgnlPublication.avgRollbackRatio

**Rollback ratio**

The percentage of publication transactions that were rolled back.

-   **What is this?** The proportion of publication commits that failed and had to be reversed.

-   **Why it matters:** A high rollback ratio indicates transaction failures that prevent content from being published.


## [](#_metric_mgnlpublication_rollbackratio)metric.mgnlPublication.rollbackRatio

**Rollback ratio over time**

Timeline showing rollback ratio trends.

-   **What is this?** A time-series graph displaying how the transaction rollback rate has changed over time.

-   **Why it matters:** Tracking rollback trends helps identify periods of transaction instability.


## [](#_metric_mgnlpublication_totaldatasize)metric.mgnlPublication.totalDataSize

**Data size**

The total volume of data transferred during publication operations.

-   **What is this?** The cumulative size of content published from Author to Public instances.

-   **Why it matters:** Data transfer volume affects network bandwidth and can impact publishing speed during high-activity periods.


## [](#_metric_mgnlpublication_datasize)metric.mgnlPublication.dataSize

**Data size over time**

Timeline showing data transfer trends.

-   **What is this?** A time-series graph displaying how publication data volume has varied over the selected period.

-   **Why it matters:** Tracking data transfer helps identify bandwidth-intensive publishing periods.


## [](#_metric_mgnlpublication_totalbyworkspace)metric.mgnlPublication.totalByWorkspace

**Total by workspace**

Publication counts for each JCR workspace.

-   **What is this?** A breakdown showing how many publications occurred in each workspace during the selected period.

-   **Why it matters:** Identifies which workspaces have the most publishing activity and helps understand content distribution.


## [](#_metric_mgnlpublication_failuresbyworkspace)metric.mgnlPublication.failuresByWorkspace

**Failures by workspace**

Failed publication counts for each JCR workspace.

-   **What is this?** A breakdown showing how many publication failures occurred in each workspace.

-   **Why it matters:** Identifies workspaces with publishing problems that may need investigation.


## [](#_metric_mgnlpublication_publishers)metric.mgnlPublication.publishers

**Top publishers**

The most active users publishing content.

-   **What is this?** A ranked list of users by publication activity during the selected period.

-   **Why it matters:** Identifies content authoring patterns and workload distribution across your team.


## [](#_metric_node_latestmemory)metric.node.latestMemory

**Latest memory**

The current memory usage for the node.

-   **What is this?** The current RAM usage for this Kubernetes node in your cluster. This represents the total memory consumed by all workloads running on the node, including Magnolia pods, system components, and other services.

-   **Why it matters:** High memory usage on a node limits the capacity available for your DXP platform workloads. When nodes run low on memory, Kubernetes may throttle or evict pods, which can cause Magnolia instances to become unresponsive, slow down content delivery, or interrupt authoring workflows. Monitoring node memory helps ensure your infrastructure has sufficient capacity to support your platform’s operations.


## [](#_metric_node_avgmemory)metric.node.avgMemory

**Average memory**

The average memory usage for the node.

-   **What is this?** The average RAM consumption over the selected time period for this Kubernetes node, representing typical memory usage across all workloads running on the node.

-   **Why it matters:** Understanding average memory usage helps you identify if your node capacity matches your platform’s needs. Sustained high averages indicate your infrastructure is operating near capacity, which may require adding nodes or scaling up node resources to maintain performance. Low averages suggest you have headroom for growth or could optimize resource allocation.


## [](#_metric_node_memory)metric.node.memory

**Memory over time**

Timeline showing memory usage trends for the node.

-   **What is this?** A time-series graph displaying how memory consumption has changed over time on this Kubernetes node, showing patterns related to workload activity, traffic patterns, or resource-intensive operations.

-   **Why it matters:** Memory trends reveal when your infrastructure is under pressure, during peak traffic periods, content publishing workflows, or when running resource-intensive operations. Identifying these patterns helps you plan infrastructure capacity, schedule maintenance, and ensure your nodes can handle both routine operations and traffic spikes without impacting your DXP platform’s performance.


## [](#_metric_node_freememory)metric.node.freeMemory

**Free memory**

The free memory for the node.

-   **What is this?** The amount of available RAM on this Kubernetes node that can be allocated to new pods or used by existing workloads when they need additional resources.

-   **Why it matters:** Low free memory means your infrastructure has limited capacity to handle increased load, new pod deployments, or traffic spikes. This can prevent Kubernetes from scheduling new workloads, cause pod evictions, and degrade performance across your DXP platform. Monitoring free memory helps ensure your infrastructure can scale to meet demand without impacting content delivery or authoring operations.


## [](#_metric_node_latestcpu)metric.node.latestCpu

**Latest CPU**

The current CPU usage for the node.

-   **What is this?** The current processor utilization for this Kubernetes node, showing how much CPU is being used by all workloads running on the node, including Magnolia pods and system processes.

-   **Why it matters:** High CPU usage on a node limits processing capacity available for your DXP platform workloads. When nodes are CPU-constrained, workloads may experience slower response times, delayed content rendering, and reduced ability to handle concurrent requests. This directly impacts both website visitors and content authors using your platform.


## [](#_metric_node_avgcpu)metric.node.avgCpu

**Average CPU**

The average CPU usage for the node.

-   **What is this?** The average processor utilization over the selected time period for this Kubernetes node, representing typical CPU load across all workloads.

-   **Why it matters:** Understanding average CPU helps you assess if your node resources match your platform’s processing needs. Consistently high averages suggest your infrastructure is working at capacity, which may require adding nodes or upgrading node resources to maintain performance. Low averages indicate you have processing headroom for growth or could optimize costs through right-sizing.


## [](#_metric_node_cpu)metric.node.cpu

**CPU over time**

Timeline showing CPU usage trends for the node.

-   **What is this?** A time-series graph displaying how CPU utilization has changed over time on this Kubernetes node, correlating with workload activity, traffic patterns, or resource-intensive operations.

-   **Why it matters:** CPU trends help you understand when your infrastructure is under stress, during peak traffic, content publishing operations, or when running intensive tasks. Identifying these patterns helps you optimize workload scheduling, plan infrastructure capacity, and ensure your nodes can handle both routine operations and peak demand without degrading your DXP platform’s performance.


## [](#_metric_node_freecpu)metric.node.freeCpu

**Free CPU**

The free CPU for the node.

-   **What is this?** The available CPU capacity on this Kubernetes node that can be allocated to new pods or used by existing workloads when they need additional processing power.

-   **Why it matters:** Low free CPU means your infrastructure has limited capacity to handle increased load, new deployments, or traffic spikes. This can prevent Kubernetes from scheduling new workloads, cause performance throttling, and degrade response times across your DXP platform. Monitoring free CPU helps ensure your infrastructure can scale to meet demand without impacting content delivery or authoring workflows.


## [](#_metric_storage_latestclusterpvcstorage)metric.storage.latestClusterPvcStorage

**Latest Persistent Volume storage**

The current Persistent Volume storage usage for the cluster.

-   **What is this?** The current storage usage for persistent volumes across your cluster. These volumes store data that persists beyond pod lifecycles, including your Magnolia content repository, digital assets, databases, and application data.

-   **Why it matters:** Persistent volume storage contains critical platform data. Running low on space can prevent new data writes, block content publishing operations, cause pod failures, and potentially lead to data loss if the system cannot write updates. Monitoring this helps ensure your infrastructure has sufficient storage capacity for content growth and can plan expansion before hitting limits that disrupt platform operations.


## [](#_metric_storage_avgclusterpvcstorage)metric.storage.avgClusterPvcStorage

**Persistent Volume storage**

The average Persistent Volume storage usage for the cluster.

-   **What is this?** The average storage consumption over the selected time period for persistent volumes across your cluster, representing typical storage usage for your platform’s persistent data.

-   **Why it matters:** Understanding average storage usage helps you track data growth trends and plan infrastructure capacity. If storage is growing rapidly, you may need to expand storage volumes, implement data archival strategies, or optimize data management before reaching capacity limits that could interrupt platform operations.


## [](#_metric_storage_clusterpvcstorage)metric.storage.clusterPvcStorage

**Persistent Volume storage over time**

Timeline showing Persistent Volume storage usage trends for the cluster.

-   **What is this?** A time-series graph displaying how persistent volume storage has changed over time across your cluster, showing data growth patterns, the impact of content migrations or imports, and storage consumption trends.

-   **Why it matters:** Storage trends reveal how quickly your platform’s persistent data is growing. Rapid growth may indicate heavy content creation, large asset uploads, or the need for data archival strategies. Understanding these patterns helps you plan storage capacity, optimize data management, and prevent storage-related issues before they impact platform operations.


## [](#_metric_storage_latestclustersystemstorage)metric.storage.latestClusterSystemStorage

**Latest system storage**

The system storage usage for the cluster.

-   **What is this?** The current storage usage for system-level data across your cluster, including container logs, temporary files, container images, and Kubernetes system data. This is separate from persistent volumes that store application data.

-   **Why it matters:** System storage fills up with logs and temporary files over time. If it runs out, it can prevent log writing, cause application errors, prevent new pod deployments, and potentially impact your platform’s ability to operate normally. Regular monitoring helps identify when log rotation, cleanup, or storage expansion is needed to maintain infrastructure health.


## [](#_metric_storage_avgclustersystemstorage)metric.storage.avgClusterSystemStorage

**System storage**

The average system storage usage for the cluster.

-   **What is this?** The average storage consumption over the selected time period for system-level storage across your cluster, including logs, temporary files, and container system data.

-   **Why it matters:** Understanding average system storage helps you identify if logs are accumulating faster than expected, which could indicate increased error rates, verbose logging, or the need for log retention policy adjustments. This helps you optimize system storage usage and ensure it doesn’t fill up unexpectedly, which could disrupt platform operations.


## [](#_metric_storage_clustersystemstorage)metric.storage.clusterSystemStorage

**System storage over time**

Timeline showing system storage usage trends for the cluster.

-   **What is this?** A time-series graph displaying how system storage has changed over time across your cluster, showing log accumulation patterns, temporary file growth, and system data trends.

-   **Why it matters:** System storage trends help you understand log growth patterns and identify when cleanup, log rotation, or storage expansion is needed. Sudden increases may indicate application issues generating excessive logs or the need for log management optimization. Monitoring these trends ensures your infrastructure maintains healthy system storage levels and prevents storage-related operational issues.

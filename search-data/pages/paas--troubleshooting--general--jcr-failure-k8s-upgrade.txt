---
title: "Magnolia JCR failure with false status"
url: https://docs.magnolia-cms.com/paas/troubleshooting/general/jcr-failure-k8s-upgrade/
category: DX Cloud
version: cloud
breadcrumb: DX Cloud > Troubleshoot > Magnolia JCR failure with false status
---

# Magnolia JCR failure with false status

Magnolia fails to connect to its database with an [exception](#_exception).

## [](#_symptoms)Symptoms

### [](#_exception)Exception

When Magnolia fails to connect to the database, an exception is thrown much like the following:

```none
info.magnolia.repository.RepositoryNotInitializedException: org.apache.jackrabbit.core.data.DataStoreException: Can not init data store, driver=org.postgresql.Driver url=null user=null schemaObjectPrefix=ds_ tableSQL=datastore createTableSQL=CREATE TABLE ds_datastore(ID VARCHAR(255) PRIMARY KEY, LENGTH BIGINT, LAST_MODIFIED BIGINT, DATA BYTEA)
...
Caused by: org.apache.commons.dbcp.SQLNestedException: Cannot create PoolableConnectionFactory (The connection attempt failed.)
...
Caused by: org.postgresql.util.PSQLException: The connection attempt failed.
...
Caused by: java.net.SocketTimeoutException: connect timed out
```

When Tomcat shuts down Magnolia, you see:

```none
SEVERE org.apache.catalina.core.StandardContext startInternal One or more listeners failed to start. Full details will be found in the appropriate container log file
```

### [](#_tomcat_receives_404)Tomcat receives 404

Requests sent to Tomcat receive a `404` since Magnolia is no longer running.

> **Note:** Requests sent to the Tomcat Magnolia REST service may work if they do not use JCR. The REST status endpoint, /.rest/status, may return a normal HTTP 200 response indicating the pod is okay.If the REST status endpoint is used as the health path (e.g., magnoliaPublic.livenessProbe.path = /.rest/status or magnoliaAuthor.livenessProbe.path = /.rest/status) the Magnolia pod passes its health and readiness probes.The REST status endpoint is the default value in the Helm chart.

## [](#_observations)Observations

The primary issue is that the pod may appear to be running normally despite Magnolia shutting down due to the exception and hence, no errors are reported in the container logs.

### [](#_when_does_this_happen)When does this happen?

The failure occurs randomly during Kubernetes upgrades. Because of this, both author and public instances are affected.

## [](#_workaround)Workaround

We are working on fixing this as it is considered a bug. As of 2026-02-18, it has not been fixed, so you should consider the workaround here.

1.  Delete the affected Magnolia instance. *You can do this via Rancher or using `kubectl` commands.*

    Magnolia `StatefulSet` recreates the Magnolia pod.

2.  Check the Magnolia logs to ensure the problem is not reoccurring.

    > **Tip:** You can do this in Magnolia using the Log Tools app.


### [](#_live_traffic)Live traffic

If the Magnolia instance is serving life traffic, the `404` response may be cached.

-   If you’re using Fastly, [purge the CDN cache](../../../cockpit/sections/content-delivery/#_cdn_statistics).

-   If you’re using a different CDN, you’ll need to flush the cache as per your CDN instructions.
